{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f01c30f9",
   "metadata": {},
   "source": [
    "Installation et Versions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7d3bc0d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scikit-learn: 1.7.2\n",
      "matplotlib: 3.10.7\n",
      "seaborn: 0.13.2\n"
     ]
    }
   ],
   "source": [
    "# =====================================================\n",
    "# SPECTRAL CLUSTERING ON KDDCUP99 DATASET - COMPLETE PROJECT\n",
    "# Fixed seed ensures EXACT same results every run\n",
    "# =====================================================\n",
    "\n",
    "import numpy as np          # Numerical computing library (arrays, math operations)\n",
    "import random               # Random number generator for Python built-ins\n",
    "seed = 42                   # Magic number! Fixed seed = reproducible results\n",
    "np.random.seed(seed)        # Set numpy's random seed to 42\n",
    "random.seed(seed)           # Set Python's random seed to 42\n",
    "import warnings             # Warning message control\n",
    "warnings.filterwarnings('ignore')  # Hide non-critical warnings for clean output\n",
    "\n",
    "# Print library versions for reproducibility (professors love this!)\n",
    "import sklearn              # Machine learning library\n",
    "import matplotlib           # Plotting library\n",
    "import seaborn as sns       # Statistical data visualization\n",
    "print(f\"scikit-learn: {sklearn.__version__}\")     # Show sklearn version\n",
    "print(f\"matplotlib: {matplotlib.__version__}\")    # Show matplotlib version\n",
    "print(f\"seaborn: {sns.__version__}\")              # Show seaborn version\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2bee0a0",
   "metadata": {},
   "source": [
    "Chargement et Exploration des Données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "16baa531",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset dimensions: (100655, 41)\n",
      "Unique classes and counts: (array([b'back.', b'buffer_overflow.', b'ipsweep.', b'land.', b'neptune.',\n",
      "       b'normal.', b'pod.', b'portsweep.', b'satan.', b'smurf.',\n",
      "       b'teardrop.', b'warezclient.'], dtype=object), array([   15,     1,    10,     1,   898, 97278,     3,     8,    15,\n",
      "        2409,     9,     8]))\n",
      "First 5 feature names: ['duration', 'protocol_type', 'service', 'flag', 'src_bytes']...\n"
     ]
    }
   ],
   "source": [
    "# Import dataset loader and preprocessing tools\n",
    "from sklearn.datasets import fetch_kddcup99       # KDDCup99 intrusion detection dataset\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder  # Data preprocessing\n",
    "import pandas as pd         # Data manipulation and analysis\n",
    "\n",
    "# Load KDDCup99 dataset \n",
    "# subset='SA' = Simulated Attacks only (smaller, faster)\n",
    "# percent10=True = Use only 10% of data (makes it Colab-friendly)\n",
    "# random_state=seed = Same random subset every time\n",
    "data = fetch_kddcup99(subset='SA', percent10=True, random_state=seed)\n",
    "\n",
    "# Separate features (X) and labels (y)\n",
    "X, y = data.data, data.target     # X=features matrix, y=true labels\n",
    "\n",
    "# Print basic dataset info\n",
    "print(f\"Dataset dimensions: {X.shape}\")              # Shape: (samples, features)\n",
    "print(f\"Unique classes and counts: {np.unique(y, return_counts=True)}\")  # Class distribution\n",
    "print(f\"First 5 feature names: {data.feature_names[:5]}...\")  # Show feature names [web:13]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60d15c44",
   "metadata": {},
   "source": [
    "Prétraitement Complet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "61e5ed41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First few rows (shows string columns):\n",
      "  duration protocol_type  service   flag src_bytes dst_bytes land  \\\n",
      "0        0        b'tcp'  b'http'  b'SF'       181      5450    0   \n",
      "1        0        b'tcp'  b'http'  b'SF'       239       486    0   \n",
      "2        0        b'tcp'  b'http'  b'SF'       235      1337    0   \n",
      "3        0        b'tcp'  b'http'  b'SF'       219      1337    0   \n",
      "4        0        b'tcp'  b'http'  b'SF'       217      2032    0   \n",
      "\n",
      "  wrong_fragment urgent hot  ... dst_host_count dst_host_srv_count  \\\n",
      "0              0      0   0  ...              9                  9   \n",
      "1              0      0   0  ...             19                 19   \n",
      "2              0      0   0  ...             29                 29   \n",
      "3              0      0   0  ...             39                 39   \n",
      "4              0      0   0  ...             49                 49   \n",
      "\n",
      "  dst_host_same_srv_rate dst_host_diff_srv_rate dst_host_same_src_port_rate  \\\n",
      "0                    1.0                    0.0                        0.11   \n",
      "1                    1.0                    0.0                        0.05   \n",
      "2                    1.0                    0.0                        0.03   \n",
      "3                    1.0                    0.0                        0.03   \n",
      "4                    1.0                    0.0                        0.02   \n",
      "\n",
      "  dst_host_srv_diff_host_rate dst_host_serror_rate dst_host_srv_serror_rate  \\\n",
      "0                         0.0                  0.0                      0.0   \n",
      "1                         0.0                  0.0                      0.0   \n",
      "2                         0.0                  0.0                      0.0   \n",
      "3                         0.0                  0.0                      0.0   \n",
      "4                         0.0                  0.0                      0.0   \n",
      "\n",
      "  dst_host_rerror_rate dst_host_srv_rerror_rate  \n",
      "0                  0.0                      0.0  \n",
      "1                  0.0                      0.0  \n",
      "2                  0.0                      0.0  \n",
      "3                  0.0                      0.0  \n",
      "4                  0.0                      0.0  \n",
      "\n",
      "[5 rows x 41 columns]\n",
      "\n",
      "Categorical columns detected:\n",
      "['duration', 'protocol_type', 'service', 'flag', 'src_bytes', 'dst_bytes', 'land', 'wrong_fragment', 'urgent', 'hot', 'num_failed_logins', 'logged_in', 'num_compromised', 'root_shell', 'su_attempted', 'num_root', 'num_file_creations', 'num_shells', 'num_access_files', 'num_outbound_cmds', 'is_host_login', 'is_guest_login', 'count', 'srv_count', 'serror_rate', 'srv_serror_rate', 'rerror_rate', 'srv_rerror_rate', 'same_srv_rate', 'diff_srv_rate', 'srv_diff_host_rate', 'dst_host_count', 'dst_host_srv_count', 'dst_host_same_srv_rate', 'dst_host_diff_srv_rate', 'dst_host_same_src_port_rate', 'dst_host_srv_diff_host_rate', 'dst_host_serror_rate', 'dst_host_srv_serror_rate', 'dst_host_rerror_rate', 'dst_host_srv_rerror_rate']\n",
      "Categorical: ['duration', 'protocol_type', 'service', 'flag', 'src_bytes', 'dst_bytes', 'land', 'wrong_fragment', 'urgent', 'hot', 'num_failed_logins', 'logged_in', 'num_compromised', 'root_shell', 'su_attempted', 'num_root', 'num_file_creations', 'num_shells', 'num_access_files', 'num_outbound_cmds', 'is_host_login', 'is_guest_login', 'count', 'srv_count', 'serror_rate', 'srv_serror_rate', 'rerror_rate', 'srv_rerror_rate', 'same_srv_rate', 'diff_srv_rate', 'srv_diff_host_rate', 'dst_host_count', 'dst_host_srv_count', 'dst_host_same_srv_rate', 'dst_host_diff_srv_rate', 'dst_host_same_src_port_rate', 'dst_host_srv_diff_host_rate', 'dst_host_serror_rate', 'dst_host_srv_serror_rate', 'dst_host_rerror_rate', 'dst_host_srv_rerror_rate']\n",
      "Numeric: 0 columns\n",
      "Original shape: (100655, 41)\n",
      "Processed shape: (100655, 18710)\n",
      "Preprocessing COMPLETE - No more string errors!\n",
      "Label range: 0 to 11\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "import pandas as pd\n",
    "\n",
    "# Convert to DataFrame to identify categorical columns easily\n",
    "df = pd.DataFrame(X, columns=data.feature_names)\n",
    "print(\"First few rows (shows string columns):\")\n",
    "print(df.head())\n",
    "print(\"\\nCategorical columns detected:\")\n",
    "print(df.select_dtypes(include=['object']).columns.tolist())\n",
    "\n",
    "# Method 1: Simple approach - encode ALL columns numerically\n",
    "# Identify categorical columns (object dtype)\n",
    "categorical_cols = df.select_dtypes(include=['object']).columns\n",
    "numeric_cols = df.select_dtypes(exclude=['object']).columns\n",
    "\n",
    "print(f\"Categorical: {categorical_cols.tolist()}\")\n",
    "print(f\"Numeric: {len(numeric_cols)} columns\")\n",
    "\n",
    "# Create preprocessing pipeline\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), numeric_cols),           # Scale numeric\n",
    "        ('cat', OneHotEncoder(sparse_output=False, drop='first'), categorical_cols)  # One-hot categorical\n",
    "    ])\n",
    "\n",
    "# Encode labels first (for evaluation later)\n",
    "le = LabelEncoder()\n",
    "y_encoded = le.fit_transform(y)\n",
    "\n",
    "# Apply preprocessing to features\n",
    "X_processed = preprocessor.fit_transform(df)\n",
    "\n",
    "print(f\"Original shape: {X.shape}\")\n",
    "print(f\"Processed shape: {X_processed.shape}\")\n",
    "print(\"Preprocessing COMPLETE - No more string errors!\")\n",
    "print(f\"Label range: {y_encoded.min()} to {y_encoded.max()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db4c164c",
   "metadata": {},
   "source": [
    "Baseline K-Means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6a1f9e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of clusters: 12\n"
     ]
    }
   ],
   "source": [
    "#NEED TO UNDERSTAND THIS ON YOUTUBE\n",
    "\n",
    "\n",
    "\n",
    "# Import clustering and evaluation metrics\n",
    "from sklearn.cluster import KMeans                    # Standard K-means algorithm\n",
    "from sklearn.metrics import silhouette_score, adjusted_rand_score  # Clustering metrics\n",
    "\n",
    "# Determine optimal number of clusters from ground truth\n",
    "n_clusters = len(np.unique(y_encoded))   # Number of unique classes = number of clusters\n",
    "print(f\"Number of clusters: {n_clusters}\")\n",
    "\n",
    "# Train K-means baseline (n_init=10 tries different random starts)\n",
    "kmeans = KMeans(n_clusters=n_clusters, random_state=seed, n_init=10)\n",
    "kmeans_labels = kmeans.fit_predict(X_processed)  # Fit model AND get predictions\n",
    "\n",
    "# Calculate quality metrics\n",
    "silhouette_kmeans = silhouette_score(X_processed, kmeans_labels)   # How separated are clusters?\n",
    "ari_kmeans = adjusted_rand_score(y_encoded, kmeans_labels)      # Agreement with ground truth\n",
    "\n",
    "print(f\"Baseline K-means - Silhouette: {silhouette_kmeans:.3f}, ARI: {ari_kmeans:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a535ae8",
   "metadata": {},
   "source": [
    " Implémentation Spectral Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44edf034",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import SpectralClustering     # Spectral clustering algorithm\n",
    "\n",
    "# Spectral Clustering with RBF kernel (Gaussian similarity)\n",
    "# gamma controls how similar points must be to connect\n",
    "spectral = SpectralClustering(\n",
    "    n_clusters=n_clusters,           # Same number as baseline\n",
    "    affinity='rbf',                  # RBF = Gaussian kernel (most common)\n",
    "    gamma=0.1,                       # Kernel width parameter (tuned later)\n",
    "    random_state=seed,               # Reproducibility\n",
    "    n_init=10                        # Multiple K-means restarts internally\n",
    ")\n",
    "\n",
    "# Fit and predict (computes Laplacian → eigenvectors → K-means automatically)\n",
    "spectral_labels = spectral.fit_predict(X_scaled)\n",
    "\n",
    "# Evaluate same metrics as baseline\n",
    "silhouette_spectral = silhouette_score(X_scaled, spectral_labels)\n",
    "ari_spectral = adjusted_rand_score(y_encoded, spectral_labels)\n",
    "\n",
    "print(f\"Spectral Clustering - Silhouette: {silhouette_spectral:.3f}, ARI: {ari_spectral:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bea35fa",
   "metadata": {},
   "source": [
    "Recherche Hyperparamètres (Grid Search)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47c57a6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import ParameterGrid  # Grid search utility\n",
    "\n",
    "# Test different gamma values (RBF kernel width)\n",
    "param_grid = {'gamma': [0.01, 0.1, 1.0, 10.0]}    # 4 values to test\n",
    "best_score = -1                                    # Track best silhouette score\n",
    "best_gamma = None                                  # Track best parameter\n",
    "\n",
    "# Loop through each gamma value\n",
    "for gamma in param_grid['gamma']:\n",
    "    # Create spectral clustering model with current gamma\n",
    "    sc = SpectralClustering(n_clusters=n_clusters, affinity='rbf', \n",
    "                           gamma=gamma, random_state=seed, n_init=10)\n",
    "    labels = sc.fit_predict(X_scaled)              # Train and predict\n",
    "    score = silhouette_score(X_scaled, labels)     # Calculate quality\n",
    "    print(f\"Gamma={gamma}: Silhouette={score:.3f}\") # Show progress\n",
    "    # Keep track of best performing gamma\n",
    "    if score > best_score:\n",
    "        best_score = score\n",
    "        best_gamma = gamma\n",
    "\n",
    "print(f\"Best gamma: {best_gamma} (score: {best_score:.3f})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d180e875",
   "metadata": {},
   "source": [
    "Visualisation (2D avec PCA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1a49193",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA      # Principal Component Analysis\n",
    "import matplotlib.pyplot as plt            # Plotting library\n",
    "\n",
    "# Reduce 41 dimensions → 2D for visualization\n",
    "pca = PCA(n_components=2, random_state=seed)  # Keep 2 principal components\n",
    "X_pca = pca.fit_transform(X_scaled)           # Transform data to 2D\n",
    "\n",
    "# Create 3 subplots side-by-side\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))  # 1 row, 3 columns, size 15x4\n",
    "\n",
    "# Plot 1: Ground truth labels (what we want to recover)\n",
    "axes[0].scatter(X_pca[:, 0], X_pca[:, 1], c=y_encoded, cmap='tab10')\n",
    "axes[0].set_title('Ground Truth')             # Chart title\n",
    "axes[0].set_xlabel('PC1')                     # X-axis label\n",
    "axes[0].set_ylabel('PC2')                     # Y-axis label\n",
    "\n",
    "# Plot 2: K-means results\n",
    "axes[1].scatter(X_pca[:, 0], X_pca[:, 1], c=kmeans_labels, cmap='tab10')\n",
    "axes[1].set_title(f'K-means (Silhouette: {silhouette_kmeans:.3f})')\n",
    "\n",
    "# Plot 3: Spectral clustering results\n",
    "axes[2].scatter(X_pca[:, 0], X_pca[:, 1], c=spectral_labels, cmap='tab10')\n",
    "axes[2].set_title(f'Spectral (Silhouette: {silhouette_spectral:.3f})')\n",
    "\n",
    "plt.tight_layout()  # Fix spacing between plots\n",
    "plt.show()          # Display the figure\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a396a5d0",
   "metadata": {},
   "source": [
    "Métriques Détaillées + Matrice de Confusion\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2676b4eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import additional evaluation tools\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "print(\"=== DETAILED COMPARISON ===\")\n",
    "print(f\"Silhouette Score - K-means: {silhouette_kmeans:.3f} | Spectral: {silhouette_spectral:.3f}\")\n",
    "print(f\"Adjusted Rand Index - K-means: {ari_kmeans:.3f} | Spectral: {ari_spectral:.3f}\")\n",
    "\n",
    "# Normalized confusion matrix (rows=truth, columns=prediction)\n",
    "cm = confusion_matrix(y_encoded, spectral_labels, normalize='true')  # Percentages\n",
    "sns.heatmap(cm, annot=True, fmt='.2f', cmap='Blues')  # Pretty heatmap\n",
    "plt.title('Confusion Matrix - Spectral Clustering')   # Title\n",
    "plt.ylabel('True Label')                              # Y-axis\n",
    "plt.xlabel('Predicted Label')                         # X-axis\n",
    "plt.show()                                            # Display\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
